# ğŸ“˜ LeetMentor â€” Your AI-Powered LeetCode Mentor (Chrome Extension)

LeetMentor is a Chrome extension that helps you solve LeetCode problems more effectively.  
It watches your solving process, detects when youâ€™re stuck, and provides **progressive hints** (Level 1 â†’ Level 3).  
After the third hint, it can optionally provide a **2â€“3 line code excerpt** to nudge you in the right direction â€” never the full solution.

LeetMentor supports two modes:

- **Local Hints Only (no AI)** â€” works out of the box  
- **AI-Generated Hints (optional)** â€” powered by **Ollama + Llama 3.2 (3B)** running locally on your machine  

This repo contains both:

ğŸ”¹ `extension/` â€” The Chrome Extension  
ğŸ”¹ `server/` â€” A Node.js server that communicates with Ollama to generate AI hints/excerpts

---

## âœ¨ Features

âœ” Smart hinting system (3 levels)  
âœ” Optional AI hints and code excerpts via Ollama  
âœ” Detects when the user is stuck (1 fail â†’ 3 min idle)  
âœ” Works directly inside the LeetCode editor  
âœ” No full solutions â€” only structured hints + small code excerpts  
âœ” Privacy-first: Code is never sent anywhere unless you enable the toggle  
âœ” Reset system resets all hint-progress cleanly  
âœ” Debug and auto-inject fallback system for reliable content-script injection  

---

# ğŸ§ª Install & Test Locally (Chrome Load Unpacked)

### 1ï¸âƒ£ Clone repo

```bash
git clone https://github.com/YOUR_USERNAME/LeetMentor.git
cd LeetMentor
```

### 2ï¸âƒ£ Load the extension in Chrome

**1.** Open chrome://extensions/

**2.** Turn on Developer mode

**3.** Click Load unpacked

**4.** Select the extension/ folder

You should now see the LeetMentor extension in your toolbar.

---
# ğŸ¤– Optional â€” Enable AI-Generated Hints (Local Ollama)

LeetMentor can generate smarter hints using a local LLM, but only if you enable and run the server + Ollama. 

This is 100% optional â€” without this, the extension still works using local fallback hints.

---

### ğŸŸ¦ Step 1 â€” Install Ollama

Ollama lets you run LLMs like Llama 3 locally.

**Windows / Linux / macOS:**

Download from:

â¡ https://ollama.com

After installation, verify:

```bash
ollama --version
```
---
### ğŸŸ© Step 2 â€” Pull the model llama3.2:3b
```
ollama pull llama3.2:3b
```
This downloads the 3B model.

---
### ğŸŸ§ Step 3 â€” Start the Ollama server
```
ollama serve
```
or
``` 
ollama daemon
```
check it's running:
```bash
curl http://localhost:11434/health
```
---
### ğŸŸ¥ Step 4 â€” Run the Node.js backend server
Open a new terminal:
```bash
cd server
npm install
node index.js
```
If everything works, you'll see:
```
LeetMentor server running on http://localhost:3000
```
The extension will now be able to request AI hints and code excerpts.

---
# âš™ï¸ Extension Settings (from Popup)

The popup contains:

### âœ” Show Hint

Shows Hint Level 1 â†’ Level 3

After Level 3, the bubble shows:

```
Reached maximum hint limit. Do you want a code excerpt?
```

### âœ” Allow code to server

When ON â†’ extension can send code snippets to `http://localhost:3000/hint`

When OFF â†’ no code is sent; local fallback hints are used

### âœ” Reset All

Resets:

* hint progress

* per-problem hint levels

* in-page hint bubbles

Does NOT reset your server settings (privacy-preserving).

---

# ğŸ”’ Privacy Notes

* Code is NEVER sent to the server unless you enable the toggle

* By default, send-to-server is OFF

* The AI backend runs locally unless you host it elsewhere

* No data is stored long-term by the extension

* No analytics or tracking exist

If you later publish to the Chrome Web Store, you must include a privacy policy that reflects these points.

---
# â— Troubleshooting
###  âŒ AI hints not working

* Check if Ollama is running

* Check server logs

* Check that popup toggle "Allow code to server" is ON

* Check background console for errors

### âŒ â€œCould not establish connectionâ€

* Content script failed to inject â€” manually refresh LeetCode page

* Background auto-inject fallback will try again

### âŒ Code excerpt shows unexpected content

* AI model may need stronger prompt rules

* Restart Ollama if streaming output freezes

---
# ğŸ¤ Contributing

Contributions welcome!

Feel free to open issues or submit PRs for new features, bug fixes, or model improvements.

---
ğŸ“„ License

MIT License

Copyright Â© 2025

---